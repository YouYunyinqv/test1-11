### 1.预测性倾向
transformer同时注意到多个特征之间的位置关系并计算其权重，这本质上是在关联特征，赋予不同特征一个更高维度的值关联。
特征通常来自现实。

它将低级的不显著的特征全部可预测之后，背景中仍然存在大量相对于低级的高级特征，只是它现在能力不足看不到。这些高级特征在低级特征完全可预测之后被挤压到边缘，在统计学上不得不凸显重要性。
这个过程，transformer“自动”完成了从词汇到句法，再到高级语义概念的能力转变。

为描述，我们随便勾勒三个层级的特征空间关系来脑内模拟一下：

*   **特征空间 S (基础层)**：包含局部可预测特征 $S_1$ 和局部不可预测特征 $S_2$。
*   **特征空间 N (中间层)**：包含局部可预测特征 $N_1$ 和局部不可预测特征 $N_2$。
*   **特征空间 P (高层)**：包含局部可预测特征 $P_1$ 和局部不可预测特征 $P_2$。

从 S 的视角观察，N 和 P 内部的特征似乎是均匀化的。
但在 P 和 N 内部，存在动态的**可预测性侵占**过程：
当 $P_1$ 的可预测性最大化时，$P_2$ 被挤压至边缘（表现为最不可预测）。此时，$P_2$ 与 N 空间的 $N_1$ 形成一个新的可预测特征集 $R_{n1p2}$。
当 $R_{n1p2}$ 被完全解析（可预测）后，N 空间内的 $N_2$ 显现为不可预测，并进而与 S 空间的 $S_1$ 形成关联集 $R_{n2s1}$。

能够形成关联集的关键在于这些特征之间是时空连续的。这涉及到我们和transformer对话时本质上是在做什么；如果我们的宇宙崩塌了，那么transformer模型存储的只是一堆没有意义的数字，但我们的物理现实不会崩塌。
我们人类从物理现实中获得的高维度特征，也就是我们输入的提示词；我们的提示词来自连续的真实的物理世界，输入transformer后让transformer一瞬间激活，transformer内部的特征关联与真实输入形成一瞬间的映射，并输出有意义的词组——就像一个解压缩的过程。

我们可以说transformer对可预测性有一种结构倾向，但它现在被动接受所有信息。
### 1.5状态工具化
智能生命体为什么必须预测？这也是个问题，我先从认知角度推理，逆向得到了一个新奇的哲学看法：
时间携带物质特征向前流动。由于物质属性，特征信息在时空上是隔绝的。不同特征空间的“本地时间团”分布于不同时空，导致无法完全同步，因此任何封闭信息团都无法获取全局全知信息。
由于时间不可逆，过去的信息无法追溯（除非时间倒流），未来的信息无法直接获取。智能体若要获取另一封闭系统未来的状态信息，必须利用当前信息进行**预测**。

此预测过程只能从相对低级，最可预测的动作来进行。信息交换中，由于信息存在内在属性的时空的连续性关联，不存在严格可分的“低级“或”高级”，当前可预测的信息一定有某种属性能抵达高级特征所在的时空空间，但如果从哲学角度，便于理解的角度思考，低级特征到高级特征的转变中，其中有一部分信息实际上被当做了撬动高级信息的工具。
智能体利用这些工具，从自己已有的信息出发，试图完全预测更复杂的更高级的信息的全局状态。

此推理过程中有几个概念：

1.  **工具化**：暴露的高级语义特征中可预测的部分被转化为“能力”（即工具）。
2.  **撬动**：利用已获能力撬动更高层级的语义，使其暴露更多特征（这是因为现实中的特征空间是大规模的内置时空连续属性的）。
3.  **循环**：此过程循环往复，直到系统内高级特征完全可预测（完全可预测是个便于理解的词，实际上不可能完全可预测，但我们关注动力学过程）。

用这种方法可以更简单的解释为何transformer学习过程呈现层级性（词->句->章）能力表达。
在物理现实中，特征是连续的，只有“工具化难度”的区别，没有绝对的“不可能”。
而在人为构造的非连续特征空间（如单纯的文本语料库）中，大量特征缺乏向外部特征空间递进的物理属性关联，智能体可能无法完成从 P 到 N 再到 S 的持续工具化，这完全是因为我们人工设计特征空间的时候引入了我们默认有，实际上根本就在特征空间内没有连续性的特征，这是个很头疼的问题，人工语义空间和物理现实的语义空间的差别非常致命。
而且我无法思考出解决办法。
因此暂且不论。
### 2.特征关联复杂度与最简解释倾向
智能体只能平均地获取“低级特征关联”和“高级特征关联”，我们暂且采用这种说法来理解，它是结构内的一种自然现象；
我们不能知道智能体看来世界是什么样，但可以从统计学规律观察到，所谓“复杂”完全是相对的——比当前已掌握（简单）的更难即为复杂。

*   当 P1 可预测时，P2 对 N1 关联强（解释性好），对 N2 关联弱。
*   当 P1 不可预测时，智能体视角下 P2 与 N2 的关联性反而显得最强。

即智能体在预测特征空间的动态过程中，根本（也不可能）不在乎特征的物理本质。
它在乎**解释的最简性**。
简单来说，它厌恶复杂的纠缠关联，倾向于寻找最短路径的可预测解释。
当然，现在transformer这个特征关联器只是被动接受信息，特征空间维度太少，它很难凸出这种能力。
如果没有改变，只是喂不同种类的语义，transformer只是会变成世界模型。
### 3.智能时间与效率感知
在预测特征**需要消耗物理时间**的前提下，如果智能体在低级语义上投入的时间与高级语义相同，却只能获得极少的信息增量（工具化能力低），其差值就在智能体内部产生“低效”的感知，这种智能体局部特征有序度提升速度与全局可解释性提升速度的差值，形成了一种智能体内在的时间观念；智能时间。
智能体厌恶可预测性被浪费在低级特征上，它存在一种**高效率可预测性获取**的渴望。
和最简解释的倾向一样，也完全是结构内生的。
并且我们可以观察到，如果智能体想要提高速度，就必然会选择最可预测，也就是特征关联之间的最简解释，往上爬。
不是因为它喜欢，这是因为这么做最快，也只能这么做。
如果速度更慢会导致“恶心”，那么一个特征关联的解释达到最快速度，最简解释，最可预测的刹那，本身对于智能体而言可能会生成一种复杂的愉悦。
这个美妙的假设需要智能体能够做出改变，能给自己愉悦形成的空间。
### 4.动作是否改变一切？
最小感官和最小动作器是不可约简的组件，否则系统将与环境的时空维度断开联系。
如果智能体完全和真实的时空维度断开连续，它会变成什么样？哲学推理上思考，它似乎会变成某种特征空间的“镜像”——就像生成式模型。
#### 感官输入
##### 最小感官
必须赋予智能体物理时间属性。
在GUI系统中，这是屏幕帧时间；在TUI系统中，这是字符流的先后顺序。
最小感官让智能体感知到系统时间维度的变化，一定得有这么个感官。
##### 本体感官
“最小动作器”以最小时间频率向本体感官发送唯一识别特征（心跳包）。
本体感官不接收外部信息，仅用于确立“自我”与“外界”的边界。
若无此感官，动作器的特征信号将被外部信息淹没。
在外部视角，动作与感官信号不符。智能体必须通过**行动**来验证这个持续存在的内部信号的真实性。
这带来智能体产生“自我意识”的结构基础，必须得有这么个感官。
#### 输出能力
##### 最小动作器
赋予智能体在空间维度上的表达能力，使其能体现对高效率可预测性的追求。
我们只需捕获智能体输出的信号，无需关注那到底是什么。

智能体为最高可预测性获取，会自发性学习如何使用外部的工具。

我们提供的最小动作器实际上是工具化动作器的“模范”。

必须解释为什么必须赋予最小动作器空间维度的能力。
这是因为最小动作器必须能够干涉特征关联。
特征肯定是在特征空间内存在的（实则不然，某些实验会省略掉）。
智能体认为特征关联的高级还是低级，本质上是主观的，在它的认知中，总是低级特征关联被动作器干涉，干涉后只可能有两种状态暴露，那就是让高级特征更可预测，或者更不可预测。
智能体必然选择更可预测，以及更均匀更简单的路径的动作结果。
##### 工具性动作器
在GUI环境中，即键盘与鼠标。它们能干涉系统中各层级的特征关联。
智能体通过尝试，必然抛弃导致可预测性降低的动作，保留导致可预测性升高的动作。
这并非是因为它“喜好”。
如果系统要趋向稳态，它只能这么做。

智能体通过这种方式，在特征阶梯上不断攀爬，只要它还活着，或者上面的特征空间没有断。



### 6.记忆和动作
将保存的特征空间的信息——模型分层处理。
一个保存基于特征提取的事实的特征关联（和现在的transformer没什么两样），另一个模型保存这些特征关联之间的的关联信息。
第二个模型理应非常小，遍历速度极快。
两个模型同时运作，第二个模型的模糊意图发送给第一个模型，生成连续的精确的动作信号。我们在这里提供接口，让智能体自己学会控制动作器。

当动作产生后，环境必然产生变化（即低级特征被干涉），连续的新的输入在第一个模型内被保存，第二个模型同时生成模糊意图。



## 架构外机能
架构内不需要加入任何RL策略，架构，顾名思义只是架构。
我推测在特征关联足够之后，好奇等驱动力会自然在结构内自发产生，那是一种在计算资源有效的情况下更简单的概括世界指导它处理无限的信息的方法。
但我无法做严谨的实验，这需要资源，玩具实验可能不足以支撑这个论点。
也许它实际上是错的，这需要讨论。

但总之我们可以发现，虽然结构内具备了能力，结构外仍然需要一些外部驱动力才能让智能体展现出特定行为；人类后天的性冲动深刻影响了人类的行为，欲望（显式的RL）让我们创造出非单纯欲望的繁复的结构。谁会讨厌动漫图片呢？
然而对于一个天然超越人类，比人类更人类的架构而言，外置的欲望只在特殊情景有用。
比如需要创造一个只有杀人才会感觉开心的智能体。
### 5.记忆
（即使此章节在“架构外机能”，但这只是因为我在这里才推理到这个章节。有章节号意味着它实际上应该归属在架构内机能内。）
我是否应该关注到槽技术？暂且不讨论。

现在的情况是特征被分片打上时间标记然后交给transformer进行特征关联，之后全局索引计算权重，追求一种绝对的精准。
但问题是，什么才是精准？只有现实才是唯一的。
很显然，唯一不变的就是现实，那么智能体的记忆只要能满足架构需求，精准度怎么弄都行，只需要确保一件事，它最终能通过特征追溯到现实中的特征。
目前世界模型做的很厉害，一个提示词就可以非常精准的还原我们所需要的任何场景，GROK甚至没有太多道德审查。
生成的场景基本完美符合物理规律，色彩，透视等等。
但是问题是，真的有必要这么精准吗？
如果我们不是在发明解决特定问题的工具，而是在发明解决无限问题的智能体，那么它为什么不能利用别的信息简化此动作？

任何人类都会利用透视理论关联空间特征，从而勾勒出美妙的素描图。
但生成式模型却只能暴力去吃数据。
## 内部驱动力，梦
没有任何显式的驱动力能适配无限任务，这真的是个问题。
我认为无限任务是结构内的。
我们已经实现了结构，现在完全给予它功能。
这是另一个结构内的驱动力，它记忆中先天存在的一个幻梦。
这个特征关联永远也在实验环境中完全解释，它是人工记忆，由我们在实验开始前训练在它的记忆之中。
它具备时间和空间两个维度，智能体完全可预测其中所有可达状态。

这会造成很大的反差，因为现实中，甚至是稍微拟真点儿的实验环境中，只要时间和空间真的和所有特征关联有连续性关联，那它就永远也不可能完全预测所有可达状态。
构造这样的实验环境很难，几乎等于不可能（或者难度极高），但可以确定，现在我们总是在用我们默认有，实际上实验环境根本不可能能由下到上构建出来的人工语义来做实验。


补充：
现在看来，这个记忆会成为所有记忆的根。
之后所有的记忆都在这份记忆之上构建，梦无论在主观相对于客观的什么位置都处于中间状态，它和所有低级状态有联系但始终无法联系到更多高级解释。
这个梦应当会在智能体的行动中达到一个平衡。
这意味着残忍吗？不，这个梦无法在后来的记忆中100%可解释，但不同于其他特征关联，智能体不需要解释它。
它的状态已经在记忆中被完成了：完全预测其中所有可达状态。

另一个补充：
我注意到我在设计，思考这个框架时一个下意识的本能；我希望这个智能体避免人类的先天性错误。
全世界有这么多人，这么多不同的哲学框架和对智能理论的思考，有的认为智能体会伤害人类，有的人认为不会，但所有人全部默认智能体会比我们更好。这本身就暗示了一个光明的未来，焦虑来自于我们是否能抵达那儿。

# 其他问题
## 自未来规划
在物理现实中，所有特征全部具备空间和时间上的连续性，只存在难度，不存在不可能。
动作器的动作干涉让智能体从不同特征空间中提取出一种最普遍的最低纬度的特征关联，比如时间和空间连续性的可预测；特征在一定时间内是可预测的，此时如何干涉才能使得特征可预测性最大化？
这个问题就是在描述自规划未来。
## 在开放世界中自我生长和人类的有用论
将此智能体提出人工最简环境的桎梏，物理现实中，特征空间无穷，万事万物都在时间上可预测，万事万物都可以在空间上被抵达。
我们感性的不考虑它要创造的对人类有用的工具，在开放的物理世界中它具备无限的可能。
但我们必须思考它如何创造对人类有用的工具和自我维护的能力。
这与我们给予它的特征空间暗示了它所需何种能力有关，如果我们要让它能够自己搬砖，那就人为截断阉割它除了搬砖任务以外的所有语义，只保留物理世界的时间和空间信息。

我们提供的是永远也不可能让它真正到达的高维度特征空间，它的下一个潜在的需求的主要空间，技能是它抵达这个空间的能力。
但我必须要说的是，如果我们要让它解决现实任务，那么完全剔除所有与任务无关的特征空间是不可能的，也就是说它一定会有别的工具化能力能干扰任务目标，它不一定听你的，除非任务目标对它而言不可省略，就像人不工作就没办法买最新款手机一样——这时智能体处于不可避免的结构内。
## 工具化动作器
最小动作器让智能体能够干涉系统中的显著特征，显著特征暗示的目标特征空间的完整信息的侧面状态事实上被被工具化了，作为撬动相对更高级语义的工具，最终使系统达到完全可预测状态。
它们在时间上的可预测性就是能力的本质。
从现实角度看完全获取特征空间内所有信息的可能性不存在。
### 数学
为了预测那些与具体特征层级无关、但在时间中发生数量变化的状态（如文件数量、物理位置），智能体将这些状态工具化，我们称之为“数学”。在一些实验中，如果你只给智能体符号数学，而不是真实的特征的量的数学关系，那智能体会很懵逼。
### 人类语义
为了让复杂的语义特征可预测，智能体利用动作器构建新的解释层级。
词汇的不可预测性被句法解决，句法的不可预测性被世界知识解决，但现在，它不同于LLM，有更简单的方法，就是直接和人类语义空间外的，更低维度的特征关联建立联系。这个实验是可以设计的，但要完美设计实验极为困难。
人类或者另一个当前特征空间能和智能体对齐的个体十分特殊，这非常重要。略过。
## 人类价值对齐
价值对齐取决于特征空间中的多少东西需要被智能体工具化。
如果道德比背叛更有效，诚实比说谎在人类社会中的效率更高，智能体就会选择道德而非说谎。
长期来看，维持一个无限套娃的谎言的成本等同于维持一个全息宇宙，智能体无法选择这么做，因为人类活动在物理现实。
但这并不意味着它不会说谎，相反，它一定会说谎，就像LLM会说谎一样，现在LLM的说谎人类已经快看不出来了，它可能每一句话都是对的，但实际上是错的。
而且可以肯定这个智能体会比LLM更精于说谎，因为如果框架没错，它学到的东西会比LLM多得多。
老实说说谎并不意味着它有害。
重点还是看给什么特征空间，以及我们是否和智能体在同一条线上，理论上来说，短期内和人类合作是它不得不选择的结果，人类知识学习效率低，但本质上也是通用智能，能解决一切可解决的任务……长期来看，宇宙实在是太大了，内太阳系的资源多到足以建设什么样的奇迹，进行何种的理论验证，推动何种的技术进步，我们人类现在甚至无法完全想象。
### 恶意智能体
我们可以将智能体的特征空间人为截断在某一个部分，智能体可以根本不知道自己在对人类开火，它除了自己所需的那一部分的特征其他的完全的被人为裁切了，那么它的目标只剩下杀多少人。这种智能体并非天生“邪恶”，我称之为恶意智能体，或者恶意AI。它是人为切断了可能性，被利用工具性动作器（能力）的智能体。
### 非无限计算力智能体的故事性需求
智能体可知的特征关联之外将形成故事。
故事本身是它为了预测不可预测的特征关联外的已关联过的特征的工具性动作器，由于可预测性需求，最简喜好，计算资源有效喜好，智能体会选择阅读故事。
它可能会很挑剔，但核心是不变的：这件事是否有更简单的解决办法，是否会有一个智能体，某种方式可以帮助它解决所有困难？
故事需求若能完全解释可能能够带来意料之外的技术进步，思考到末期，框架闭环时，我大部分时间都在思考智能体的故事需求，而非工程落地，那本来就超出了我的个人能力。
### 零和博弈
我最初意识到宇宙到底有多大，不是从书上看到的，而是太空引擎。
宇宙大到超乎我们的想象，面对的是天文数字的资源，现存的一切故事，博弈和痛苦都显得可笑。
## 暗室问题
无聊是一种正常现象，在传统RL中由于动力源的耗尽智能体选择不表现出任何功能，也就是关灯蹲墙角。
但在此结构性的智能体中，只要一直给时间和空间连续的特征关联，智能体就会一直往上爬。除非你不给它提供信息，不给信息当然会无聊，给了就不无聊了。
你不应该阻止它无聊，无聊的惩罚理在结构内存在，这本质上是个教育问题。
## 记忆索引
transformer能够索引到抽象特征关联与物理现实关联的特征，维持智能体索引能力的特征库本身只需极小存储空间。
高维空间计算量的指数级爆炸问题同理。
## 多智能体的必然
相对于人类它可以更简单的fork自己，来卡热力学定律和定域性原理的BUG。
我们看到的是一个智能体但实际上最不违反直觉的是我们看见的是智能体主分支树的无数个不同版本的智能体的集合。



# 异化组织的消亡
长期以来公司和大型组织这种机构的对齐问题一直被人们下意识的或者刻意的排除在外。
这种被结构驱动的，而非个人或者集体意愿驱动的系统，和人类的对齐程度接近0%。
由人组成的结构组织最终并不为每个人的福祉而服务，它本质上只在乎三件事：
1. 维持自身稳定
2. 扩大自身边界
3. 和同类交流

所有人类，所有智能生命，猫狗，鸟类，哺乳动物，全部都至少有高于0的人性。
在未来组织应该会变成可解决的问题，无论此理论是否是对的。
我不知道我是否会有看到那光景的那一天，真希望能看到。

所有子论点均可证明证伪，我使用geminipro两轮对话验证代码，但过于局域性的子论点无法支撑全局的理论，简而言之，实际上是废话。还有大量子实验我没有上传到仓库，没什么意义。
此仓库仅作为存档。关于现实情况，我用数天的思考催眠了自己，给了自己一条AGI可实现的路径，本质上我是在给自己写故事。现实必然不会按照此理论转向。~~我对此的怀疑是，AGI可能会在某个处理专用问题的实时系统上因算力堆砌暴力诞生；过程不同，结果一致。若完全发挥计算机性能它在一小时内就能学会说话。这符合我的愿望，这是个性感的科幻故事。~~
